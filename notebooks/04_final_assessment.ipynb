{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cc713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload in Jupyter\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b930c88",
   "metadata": {},
   "source": [
    "# Imports and Seed Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da2b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All random seeds set to 51 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables for reproducibility BEFORE importing torch\n",
    "os.environ['PYTHONHASHSEED'] = '51'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path for module imports\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import fiftyone as fo\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.datasets import CustomTorchImageDataset\n",
    "from src.models import (\n",
    "    ContrastivePretraining,\n",
    "    Embedder,\n",
    "    Projector,\n",
    "    RGB2LiDARClassifier,\n",
    ")\n",
    "from src.training import train_model\n",
    "from src.utils import (\n",
    "    set_seeds,\n",
    "    create_deterministic_training_dataloader,\n",
    "    get_rgb_input,\n",
    "    get_mm_intermediate_inputs,\n",
    "    infer_model,\n",
    ")\n",
    "\n",
    "set_seeds(51)\n",
    "\n",
    "PROJECT_NAME = \"cilp-extended-assessment\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14535b3",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b3bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing samples...\n",
      " 100% |███████████████| 3228/3228 [95.0ms elapsed, 0s remaining, 34.0K samples/s]   \n",
      "Total samples in dataset: 1076\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 64\n",
    "\n",
    "dataset_name = \"cilp_assessment\"\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=Path.cwd().parent / dataset_name,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")\n",
    "\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed4176",
   "metadata": {},
   "source": [
    "Extract train and test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4914f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 897\n",
      "Validation samples: 179\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset.match_tags(\"train\")\n",
    "val_dataset = dataset.match_tags(\"validation\")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9932e27",
   "metadata": {},
   "source": [
    "Generate custom torch datasets to use dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef610b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "CustomTorchImageDataset initialized with 897 samples.\n",
      "CustomTorchImageDataset initialized with 179 samples.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device: \", device)\n",
    "\n",
    "torch_train_dataset = CustomTorchImageDataset(\n",
    "    fiftyone_dataset=train_dataset,\n",
    "    img_size=IMG_SIZE,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "torch_val_dataset = CustomTorchImageDataset(\n",
    "    fiftyone_dataset=val_dataset,\n",
    "    img_size=IMG_SIZE,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bf39a",
   "metadata": {},
   "source": [
    "Create a DataLoader and use a deterministic setup for training to make the results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f0fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_deterministic_training_dataloader(\n",
    "    torch_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    torch_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10d017",
   "metadata": {},
   "source": [
    "# Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6f712",
   "metadata": {},
   "source": [
    "First, we create our embedder for RGB and Lidar data and train them using contrastive pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d7fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "\n",
    "img_embedder = Embedder(in_ch=4, emb_size=CILP_EMB_SIZE).to(device)\n",
    "lidar_embedder = Embedder(in_ch=4, emb_size=CILP_EMB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46418fc",
   "metadata": {},
   "source": [
    "We define a custom loss function for contrastive pretraining that aligns the embeddings of the two modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939c0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive Loss for matching embeddings from two modalities.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): A tuple containing image embeddings and lidar embeddings.\n",
    "        _: Placeholder for compatibility.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Computed contrastive loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.loss_img = nn.CrossEntropyLoss()\n",
    "        self.loss_lidar = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, _) -> torch.Tensor:\n",
    "        img_embeddings, lidar_embeddings = embeddings\n",
    "        \n",
    "        batch_size = img_embeddings.size(0)\n",
    "        ground_truth = torch.arange(batch_size, dtype=torch.long).to(device)\n",
    "\n",
    "        loss_img = self.loss_img(img_embeddings, ground_truth)\n",
    "        loss_lidar = self.loss_lidar(lidar_embeddings, ground_truth)\n",
    "        loss = (loss_img + loss_lidar) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65dac2b",
   "metadata": {},
   "source": [
    "We use contrastive pretraining to pretrain our embedder for RGB and Lidar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bcc595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matti/Master/3. Semester/Hands-on Computer Vision/lab2/CILP-Assessment-Multimodal-Learning/notebooks/wandb/run-20251226_134916-7heq4qwr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/7heq4qwr' target=\"_blank\">ContrastivePretraining</a></strong> to <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/7heq4qwr' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/7heq4qwr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training contrastive pretraining model...\n",
      "All random seeds set to 51 for reproducibility\n",
      "Validation loss:  3.378854274749756\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "CILP_model = ContrastivePretraining(img_embedder, lidar_embedder).to(device)\n",
    "optimizer = Adam(CILP_model.parameters(), lr=0.0001)\n",
    "loss_func = ContrastiveLoss()\n",
    "\n",
    "cilp_save_path = Path.cwd().parent / \"checkpoints\" / \"04_cilp_contrastive_best.pth\"\n",
    "mm_cilp_run = wandb.init(project=PROJECT_NAME, name=f\"{ContrastivePretraining.__name__}\")\n",
    "\n",
    "print(\"Training contrastive pretraining model...\")\n",
    "set_seeds(51)\n",
    "mm_cilp_train_loss, mm_cilp_valid_loss, mm_cilp_train_time = train_model(\n",
    "    CILP_model,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    get_mm_intermediate_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    save_path=cilp_save_path,\n",
    "    run=mm_cilp_run,\n",
    "    log_predictions=False,\n",
    ")\n",
    "\n",
    "print(\"Validation loss: \", np.min(mm_cilp_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27469b2",
   "metadata": {},
   "source": [
    "We load the best-performing model and freeze all of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8126f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_model.load_state_dict(torch.load(cilp_save_path))\n",
    "\n",
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17888ce3",
   "metadata": {},
   "source": [
    "## Computation of Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71196c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_embeddings = []\n",
    "val_lidar_embeddings = []\n",
    "\n",
    "CILP_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        img_inputs, lidar_inputs = get_mm_intermediate_inputs(batch)\n",
    "        img_embeds = CILP_model.img_embedder(img_inputs)\n",
    "        lidar_embeds = CILP_model.lidar_embedder(lidar_inputs)\n",
    "        val_img_embeddings.append(img_embeds.cpu().numpy())\n",
    "        val_lidar_embeddings.append(lidar_embeds.cpu().numpy())\n",
    "        \n",
    "val_img_embeddings = np.vstack(val_img_embeddings)\n",
    "val_lidar_embeddings = np.vstack(val_lidar_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ca3a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed validation embeddings shape:  (179, 200)\n",
      "Computed validation embeddings shape:  (179, 200)\n",
      "Similarity matrix shape:  (179, 179)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>learning_rate</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_loss</td><td>3.34608</td></tr><tr><td>valid_loss</td><td>3.37913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ContrastivePretraining</strong> at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/7heq4qwr' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/7heq4qwr</a><br> View project at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_134916-7heq4qwr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Computed validation embeddings shape: \", val_img_embeddings.shape)\n",
    "print(\"Computed validation embeddings shape: \", val_lidar_embeddings.shape)\n",
    "\n",
    "sim_matrix = val_img_embeddings @ val_img_embeddings.T  # Cosine similarity\n",
    "\n",
    "print(\"Similarity matrix shape: \", sim_matrix.shape)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sim_matrix, cmap=\"viridis\")\n",
    "plt.title(\"Embedding Cosine Similarity\")\n",
    "mm_cilp_run.log({\"similarity_matrix\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "mm_cilp_run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801d164",
   "metadata": {},
   "source": [
    "# Cross-Modal Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbaa6ec",
   "metadata": {},
   "source": [
    "The projector takes embedded RGB features as input. We obtain these embeddings using the best-performing RGB encoder from the contrastive pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f041b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_inputs(batch):\n",
    "    \"\"\"\n",
    "    Get the inputs for the projector model.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch of data containing RGB images and LiDAR depth maps.\n",
    "\n",
    "    Returns:\n",
    "        List of image embeddings obtained from the CILP model's image embedder.\n",
    "    \"\"\"\n",
    "    rbg_img, _, _ = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img).to(device)\n",
    "    \n",
    "    return [imb_embs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641254d",
   "metadata": {},
   "source": [
    "For the loss computation, we compare the projected RGB embeddings with the corresponding LiDAR embeddings. To simplify the training loop, we introduce an auxiliary loss function that computes the LiDAR embeddings on the fly. Alternatively, one could precompute the embeddings and construct a separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c867ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectorLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Projector Loss to align projected image embeddings with LiDAR embeddings.   \n",
    "    Uses Mean Squared Error (MSE) loss.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): A tuple containing image embeddings and lidar embeddings.\n",
    "        _: Placeholder for compatibility.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Computed MSE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ProjectorLoss, self).__init__()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, lidar_data: torch.Tensor) -> torch.Tensor:\n",
    "        # Get LiDAR embeddings from the CILP model\n",
    "        lidar_embeddings = CILP_model.lidar_embedder(lidar_data).to(device)\n",
    "        loss = self.loss_func(embeddings, lidar_embeddings)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233b4d2",
   "metadata": {},
   "source": [
    "We train a projector that maps RGB embeddings to LiDAR embeddings. Since this is a regression task, we use a mean squared error (MSE) loss. We need an own loss function to compare embeddings and not the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec8bdf7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matti/Master/3. Semester/Hands-on Computer Vision/lab2/CILP-Assessment-Multimodal-Learning/notebooks/wandb/run-20251226_134943-eg7yezze</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/eg7yezze' target=\"_blank\">Projector</a></strong> to <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/eg7yezze' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/eg7yezze</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training projector model...\n",
      "All random seeds set to 51 for reproducibility\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>learning_rate</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_loss</td><td>1e-05</td></tr><tr><td>valid_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Projector</strong> at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/eg7yezze' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/eg7yezze</a><br> View project at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_134943-eg7yezze/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  4.3251088754914235e-06\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "projector = Projector(in_emb_size=CILP_EMB_SIZE, out_emb_size=CILP_EMB_SIZE)\n",
    "projector_opt = torch.optim.Adam(projector.parameters())\n",
    "# We want to minimize the MSE between the projected RGB embeddings and the Lidar embeddings\n",
    "projector_loss_func = ProjectorLoss()\n",
    "\n",
    "projector_save_path = Path.cwd().parent / \"checkpoints\" / \"04_mm_projector_best.pth\"\n",
    "projector_run = wandb.init(project=PROJECT_NAME, name=f\"{Projector.__name__}\")\n",
    "\n",
    "print(\"Training projector model...\")\n",
    "set_seeds(51)\n",
    "mm_projector_train_loss, mm_projector_valid_loss, mm_projector_train_time = train_model(\n",
    "    projector,\n",
    "    projector_opt,\n",
    "    projector_loss_func,\n",
    "    get_projector_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    target_idx=1, # We want to predict lidar embeddings\n",
    "    log_predictions=False,\n",
    "    save_path=projector_save_path,\n",
    "    run=projector_run,\n",
    ")\n",
    "\n",
    "projector_run.finish()\n",
    "\n",
    "print(\"Validation loss: \", np.min(mm_projector_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1c8fe",
   "metadata": {},
   "source": [
    "# Final Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f92ade",
   "metadata": {},
   "source": [
    "Load the CILP and projector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2421ef7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CILP_model.load_state_dict(torch.load(cilp_save_path))\n",
    "projector.load_state_dict(torch.load(projector_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb7d6c",
   "metadata": {},
   "source": [
    "We train the RGB2LiDARClassifier, which embeds and projects RGB images into the LiDAR embedding space and then applies a lightweight LiDAR-based classifier to produce the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c09241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matti/Master/3. Semester/Hands-on Computer Vision/lab2/CILP-Assessment-Multimodal-Learning/notebooks/wandb/run-20251226_135006-zi5pbetg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/zi5pbetg' target=\"_blank\">RGB2LiDARClassifier</a></strong> to <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/zi5pbetg' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/zi5pbetg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RGB2LiDARClassifier model...\n",
      "All random seeds set to 51 for reproducibility\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>learning_rate</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>train_loss</td><td>0.23367</td></tr><tr><td>valid_loss</td><td>0.20535</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RGB2LiDARClassifier</strong> at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/zi5pbetg' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/zi5pbetg</a><br> View project at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251226_135006-zi5pbetg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.2053543490668138\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "rgb_2_lidar_classifier = RGB2LiDARClassifier(\n",
    "    img_embedder=CILP_model.img_embedder,\n",
    "    projector=projector,\n",
    ")\n",
    "rgb_2_lidar_classifier_opt = torch.optim.Adam(rgb_2_lidar_classifier.parameters())\n",
    "bce_loss_func = nn.BCEWithLogitsLoss()\n",
    "rgb_2_lidar_save_path = Path.cwd().parent / \"checkpoints\" / \"04_rgb2lidar_classifier.pth\"\n",
    "rgb_2_lidar_run = wandb.init(project=PROJECT_NAME, name=f\"{RGB2LiDARClassifier.__name__}\")\n",
    "\n",
    "print(\"Training RGB2LiDARClassifier model...\")\n",
    "set_seeds(51)\n",
    "mm_rgb2lidar_train_loss, mm_rgb2lidar_valid_loss, mm_rgb2lidar_train_time = train_model(\n",
    "    rgb_2_lidar_classifier,\n",
    "    rgb_2_lidar_classifier_opt,\n",
    "    bce_loss_func,\n",
    "    get_rgb_input,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    save_path=rgb_2_lidar_save_path,\n",
    "    run=rgb_2_lidar_run,\n",
    ")\n",
    "\n",
    "rgb_2_lidar_run.finish()\n",
    "\n",
    "print(\"Validation loss: \", np.min(mm_rgb2lidar_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00fc9c",
   "metadata": {},
   "source": [
    "Create a concatinated dataset for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91aaa49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in concat dataset: 1076\n"
     ]
    }
   ],
   "source": [
    "concat_dataset = ConcatDataset([torch_train_dataset, torch_val_dataset])\n",
    "print(f\"Total samples in concat dataset: {len(concat_dataset)}\")\n",
    "\n",
    "concat_dataloader = DataLoader(\n",
    "    concat_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fc93b",
   "metadata": {},
   "source": [
    "Load best model and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8ac3ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy on combined train and validation set: 0.28%\n"
     ]
    }
   ],
   "source": [
    "rgb_2_lidar_classifier.load_state_dict(torch.load(rgb_2_lidar_save_path))\n",
    "\n",
    "accuracy, _ = infer_model(\n",
    "    rgb_2_lidar_classifier,\n",
    "    concat_dataloader,\n",
    "    get_rgb_input,\n",
    ")\n",
    "\n",
    "print(f\"Final accuracy on combined train and validation set: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
