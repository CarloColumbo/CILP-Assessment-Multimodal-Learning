{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cc713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload in Jupyter\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b930c88",
   "metadata": {},
   "source": [
    "# Imports and Seed Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2b12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All random seeds set to 51 for reproducibility\n",
      "All random seeds set to 51 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables for reproducibility BEFORE importing torch\n",
    "os.environ['PYTHONHASHSEED'] = '51'\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path for module imports\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import fiftyone as fo\n",
    "from torch.optim import Adam\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets import CustomTorchImageDataset\n",
    "from src.models import (\n",
    "    ContrastivePretraining,\n",
    "    Embedder,\n",
    "    Projector,\n",
    "    RGB2LiDARClassifier,\n",
    ")\n",
    "from src.training import train_model\n",
    "from src.utils import (\n",
    "    set_seeds,\n",
    "    create_deterministic_training_dataloader,\n",
    "    get_rgb_input,\n",
    "    get_mm_intermediate_inputs,\n",
    "    infer_model,\n",
    ")\n",
    "\n",
    "set_seeds(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14535b3",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b3bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing samples...\n",
      " 100% |█████████████| 32253/32253 [1.0s elapsed, 0s remaining, 31.6K samples/s]         \n",
      "Total samples in dataset: 10751\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 64\n",
    "\n",
    "dataset_name = \"cilp_assessment\"\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=Path.cwd().parent / dataset_name,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")\n",
    "\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed4176",
   "metadata": {},
   "source": [
    "Extract train and test split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4914f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 895\n",
      "Validation samples: 179\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset.match_tags(\"train\")\n",
    "val_dataset = dataset.match_tags(\"validation\")\n",
    "\n",
    "# select 10% of both\n",
    "train_dataset = train_dataset.take(int(0.1 * len(train_dataset)), seed=51)\n",
    "val_dataset = val_dataset.take(int(0.1 * len(val_dataset)), seed=51)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9932e27",
   "metadata": {},
   "source": [
    "Generate custom torch datasets to use dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef610b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTorchImageDataset initialized with 895 samples.\n",
      "CustomTorchImageDataset initialized with 179 samples.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device: \", device)\n",
    "\n",
    "torch_train_dataset = CustomTorchImageDataset(\n",
    "    fiftyone_dataset=train_dataset,\n",
    "    img_size=IMG_SIZE,\n",
    ")\n",
    "\n",
    "torch_val_dataset = CustomTorchImageDataset(\n",
    "    fiftyone_dataset=val_dataset,\n",
    "    img_size=IMG_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5bf39a",
   "metadata": {},
   "source": [
    "Create a DataLoader and use a deterministic setup for training to make the results reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_deterministic_training_dataloader(\n",
    "    torch_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    torch_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10d017",
   "metadata": {},
   "source": [
    "# Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6f712",
   "metadata": {},
   "source": [
    "First, we create our embedder for RGB and Lidar data and train them using contrastive pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "\n",
    "img_embedder = Embedder(in_ch=4, emb_size=CILP_EMB_SIZE).to(device)\n",
    "lidar_embedder = Embedder(in_ch=4, emb_size=CILP_EMB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46418fc",
   "metadata": {},
   "source": [
    "We define a custom loss function for contrastive pretraining that aligns the embeddings of the two modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive Loss for matching embeddings from two modalities.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): A tuple containing image embeddings and lidar embeddings.\n",
    "        _: Placeholder for compatibility.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Computed contrastive loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.loss_img = nn.CrossEntropyLoss()\n",
    "        self.loss_lidar = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, _) -> torch.Tensor:\n",
    "        img_embeddings, lidar_embeddings = embeddings\n",
    "        \n",
    "        batch_size = img_embeddings.size(0)\n",
    "        ground_truth = torch.arange(batch_size, dtype=torch.long).to(device)\n",
    "\n",
    "        loss_img = self.loss_img(img_embeddings, ground_truth)\n",
    "        loss_lidar = self.loss_lidar(lidar_embeddings, ground_truth)\n",
    "        loss = (loss_img + loss_lidar) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65dac2b",
   "metadata": {},
   "source": [
    "We use contrastive pretraining to pretrain our embedder for RGB and Lidar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training contrastive pretraining model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarl-schuetz\u001b[0m (\u001b[33mkarl-schuetz-hasso-plattner-institut\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/matti/Master/3. Semester/Hands-on Computer Vision/lab2/project/notebooks/wandb/run-20251218_112141-qwuychsg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/qwuychsg' target=\"_blank\">ContrastivePretraining_training</a></strong> to <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/qwuychsg' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/qwuychsg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>learning_rate</td><td>▁▁</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_loss</td><td>3.46436</td></tr><tr><td>valid_loss</td><td>3.37909</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ContrastivePretraining_training</strong> at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/qwuychsg' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment/runs/qwuychsg</a><br> View project at: <a href='https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/karl-schuetz-hasso-plattner-institut/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251218_112141-qwuychsg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "CILP_model = ContrastivePretraining(img_embedder, lidar_embedder).to(device)\n",
    "optimizer = Adam(CILP_model.parameters(), lr=0.0001)\n",
    "loss_func = ContrastiveLoss()\n",
    "\n",
    "cilp_save_path = Path.cwd().parent / \"checkpoints\" / \"04_cilp_contrastive_best.pth\"\n",
    "\n",
    "print(\"Training contrastive pretraining model...\")\n",
    "set_seeds(51)\n",
    "mm_cilp_train_loss, mm_cilp_valid_loss, mm_cilp_train_time = train_model(\n",
    "    CILP_model,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    get_mm_intermediate_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    save_path=cilp_save_path,\n",
    ")\n",
    "\n",
    "print(\"Validation loss: \", np.min(mm_cilp_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27469b2",
   "metadata": {},
   "source": [
    "We load the best-performing model and freeze all of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_model.load_state_dict(torch.load(cilp_save_path))\n",
    "\n",
    "for param in CILP_model.parameters():\n",
    "    CILP_model.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801d164",
   "metadata": {},
   "source": [
    "# Cross-Modal Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbaa6ec",
   "metadata": {},
   "source": [
    "The projector takes embedded RGB features as input. We obtain these embeddings using the best-performing RGB encoder from the contrastive pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projector_inputs(batch):\n",
    "    \"\"\"\n",
    "    Get the inputs for the projector model.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch of data containing RGB images and LiDAR depth maps.\n",
    "\n",
    "    Returns:\n",
    "        List of image embeddings obtained from the CILP model's image embedder.\n",
    "    \"\"\"\n",
    "    rbg_img, _, _ = batch\n",
    "    imb_embs = CILP_model.img_embedder(rbg_img).to(device)\n",
    "    \n",
    "    return [imb_embs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641254d",
   "metadata": {},
   "source": [
    "For the loss computation, we compare the projected RGB embeddings with the corresponding LiDAR embeddings. To simplify the training loop, we introduce an auxiliary loss function that computes the LiDAR embeddings on the fly. Alternatively, one could precompute the embeddings and construct a separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c867ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectorLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Projector Loss to align projected image embeddings with LiDAR embeddings.   \n",
    "    Uses Mean Squared Error (MSE) loss.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): A tuple containing image embeddings and lidar embeddings.\n",
    "        _: Placeholder for compatibility.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Computed MSE loss.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ProjectorLoss, self).__init__()\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, lidar_data: torch.Tensor) -> torch.Tensor:\n",
    "        # Get LiDAR embeddings from the CILP model\n",
    "        lidar_embeddings = CILP_model.lidar_embedder(lidar_data).to(device)\n",
    "        loss = self.loss_func(embeddings, lidar_embeddings)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233b4d2",
   "metadata": {},
   "source": [
    "We train a projector that maps RGB embeddings to LiDAR embeddings. Since this is a regression task, we use a mean squared error (MSE) loss. We need an own loss function to compare embeddings and not the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bdf7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2119248113.py, line 16)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtarget_idx=1 # We want to predict lidar embeddings\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "projector = Projector(in_emb_size=CILP_EMB_SIZE, out_emb_size=CILP_EMB_SIZE)\n",
    "projector_opt = torch.optim.Adam(projector.parameters())\n",
    "# We want to minimize the MSE between the projected RGB embeddings and the Lidar embeddings\n",
    "projector_loss_func = ProjectorLoss()\n",
    "\n",
    "projector_save_path = Path.cwd().parent / \"checkpoints\" / \"04_mm_projector_best.pth\"\n",
    "\n",
    "print(\"Training projector model...\")\n",
    "set_seeds(51)\n",
    "mm_projector_train_loss, mm_projector_valid_loss, mm_projector_train_time = train_model(\n",
    "    projector,\n",
    "    projector_opt,\n",
    "    projector_loss_func,\n",
    "    get_projector_inputs,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    target_idx=1, # We want to predict lidar embeddings\n",
    "    save_path=projector_save_path,\n",
    ")\n",
    "\n",
    "print(\"Validation loss: \", np.min(mm_projector_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1c8fe",
   "metadata": {},
   "source": [
    "# Final Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f92ade",
   "metadata": {},
   "source": [
    "Load the CILP and projector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_model.load_state_dict(torch.load(cilp_save_path))\n",
    "projector.load_state_dict(torch.load(projector_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb7d6c",
   "metadata": {},
   "source": [
    "We train the RGB2LiDARClassifier, which embeds and projects RGB images into the LiDAR embedding space and then applies a lightweight LiDAR-based classifier to produce the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c09241",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "rgb_2_lidar_classifier = RGB2LiDARClassifier(\n",
    "    img_embedder=CILP_model.img_embedder,\n",
    "    projector=projector,\n",
    ")\n",
    "rgb_2_lidar_classifier_opt = torch.optim.Adam(rgb_2_lidar_classifier.parameters())\n",
    "bce_loss_func = nn.BCEWithLogitsLoss()\n",
    "rgb_2_lidar_save_path = Path.cwd().parent / \"checkpoints\" / \"04_rgb2lidar_classifier.pth\"\n",
    "\n",
    "print(\"Training RGB2LiDARClassifier model...\")\n",
    "set_seeds(51)\n",
    "mm_rgb2lidar_train_loss, mm_rgb2lidar_valid_loss, mm_rgb2lidar_train_time = train_model(\n",
    "    rgb_2_lidar_classifier,\n",
    "    rgb_2_lidar_classifier_opt,\n",
    "    bce_loss_func,\n",
    "    get_rgb_input,\n",
    "    epochs,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    save_path=rgb_2_lidar_save_path,\n",
    ")\n",
    "\n",
    "print(\"Validation loss: \", np.min(mm_rgb2lidar_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00fc9c",
   "metadata": {},
   "source": [
    "Create a concatinated dataset for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaa49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dataset = ConcatDataset([torch_train_dataset, torch_val_dataset])\n",
    "print(f\"Total samples in concat dataset: {len(concat_dataset)}\")\n",
    "\n",
    "concat_dataloader = DataLoader(\n",
    "    concat_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fc93b",
   "metadata": {},
   "source": [
    "Load best model and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac3ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_2_lidar_classifier.load_state_dict(torch.load(rgb_2_lidar_save_path))\n",
    "\n",
    "accuracy, _ = infer_model(\n",
    "    rgb_2_lidar_classifier,\n",
    "    concat_dataloader,\n",
    "    get_rgb_input,\n",
    ")\n",
    "\n",
    "print(f\"Final accuracy on combined train and validation set: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
